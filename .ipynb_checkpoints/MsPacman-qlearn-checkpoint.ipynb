{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MsPacman Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a model to play Ms Pacman through reinforcement learning. \n",
    "For this purpose, we use the OpenAI Gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-22 23:08:04,504] Making new env: MsPacman-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "#Use MsPacman\n",
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an agent decission problem (?) we need a set of possible actions the agent can perform. The environment we created has a set of possible actions, and we can see their meaning in the context of an Atari game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, we decide an action, and this brings with it a new state, a reward, if the game is over, and more information on the game (like if we have more lifes left).\n",
    "\n",
    "Let's see what would happen if the agent always choses random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "50.0 False 2\n",
      "10.0 False 2\n",
      "10.0 False 2\n",
      "200.0 False 2\n",
      "10.0 False 1\n",
      "10.0 False 1\n",
      "10.0 False 1\n",
      "10.0 False 1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(1000):\n",
    "    action = np.random.choice(8)\n",
    "    obs, reward, game_over, info = env.step(action)\n",
    "#     env.render()\n",
    "    if reward > 0:\n",
    "        print(reward,game_over,info['ale.lives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid learning noise, we will make the space of actions smaller, and remove the composite actions. Leaving only left, right, up and down, and noop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what each state looks like in terms of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "160\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(len(obs))\n",
    "print(len(obs[0]))\n",
    "print(len(obs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "\n",
    "Before training, we can convert our board to images that actually matter for the game. We can also turn it into a square image, if no information is lost\n",
    "\n",
    "This is how the board looks at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe323e04a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "original_board = env.reset()\n",
    "plt.imshow(original_board)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the lower rectangle that has the game statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3205ce9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cropped = original_board[0:170]\n",
    "plt.imshow(cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make it blurry using nearest neighbour image interpolation to resie the image to half the size. The ghosts are still there, so are the points, pacman and the walls. We can also convert it to gray scale to not have 3 components of RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe371a058d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.misc import imresize\n",
    "scaled = cropped.mean(axis=2) # convert to grayscale\n",
    "scaled = scaled/255 #normalize\n",
    "scaled = imresize(scaled, size=(85,85), interp='nearest')\n",
    "plt.imshow(scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we create our first constant and helping function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 85\n",
    "\n",
    "def rescale_board(board):\n",
    "    cropped = board[0:170]\n",
    "    scaled = cropped.mean(axis=2)\n",
    "    return imresize(scaled/255, size=(BOARD_SIZE, BOARD_SIZE), interp='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition and State Update\n",
    "\n",
    "To train the network, we don't use just the immediate previous screen, we need a set of screens. This way, the network can better make inference on things like speed and direction of moving objects on the game's screen.\n",
    "\n",
    "In the paper used for this project, the authors used 4 screens to define the \"state\". We'll do the same.\n",
    "\n",
    "We define a function to update the state, which will take as an input the current state and the new observation, and it will remove the oldest observation, and introduce the new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(current_state, new_obs):\n",
    "    scaled_obs = rescale_board(new_obs)\n",
    "    return np.append(state[1:], np.expand_dims(scaled_obs, 0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Before doing anything else, we'll start creating our buffer for experience replay.\n",
    "\n",
    "Experience replay is an improvement done to deep q-learning. Essentialy, you accumulate experiences (states) from game play. This plays can be done by an expert player, or at random. They are useful because they eliminate the time-dependencies introduced by the fact that the game frames always follow a sequence.\n",
    "\n",
    "We generate our buffer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_exec = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(real_exec)\n",
    "MIN_REPLAY_EXPERIENCES = 30000 if real_exec else 40\n",
    "MAX_REPLAY_EXPERIENCES = 300000 if real_exec else 200\n",
    "\n",
    "print(MIN_REPLAY_EXPERIENCES)\n",
    "\n",
    "s = 4 # number of screens in a state\n",
    "experience_buffer = []\n",
    "K = env.action_space.n - 4 # We will remove the mixed actions, because they may be noisy\n",
    "\n",
    "initial_obs = env.reset()\n",
    "scaled_initial_obs = rescale_board(initial_obs)\n",
    "state = np.stack([scaled_initial_obs] * s, axis=0)\n",
    "for i in range(MIN_REPLAY_EXPERIENCES):\n",
    "    action = np.random.choice(K)\n",
    "    obs, reward, game_over, _ = env.step(action)\n",
    "    next_state = update_state(state, obs)\n",
    "    experience_buffer.append((state, action, reward, next_state, game_over))\n",
    "    if game_over:\n",
    "        obs = env.reset()\n",
    "        scaled_obs = rescale_board(obs)\n",
    "        state = np.stack([scaled_obs] * s, axis=0)\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Definition\n",
    "\n",
    "Now we define the Neural Network using tensorflow. We define the network as a class to avoid bugs due to variable passing. This network class will have the network definition, plus wrapping methos to predict, update state and select actions. At this time, we also define the training parameters for the network (i.e. how many episodes we play, epsilon and it's decay, gamma, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "## Training params definition\n",
    "batch_size = 30 if real_exec else 10 #for real 30, for testing 10\n",
    "num_episodes = 10000 if real_exec else 50 # for real 10000 for testing 25\n",
    "gamma = 0.90\n",
    "episode_rewards = np.zeros(num_episodes)\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_step = 500000 if real_exec else 10000\n",
    "epsilon_delta = (epsilon - epsilon_min) / epsilon_step # for real divide by 500000, for testing by 10000\n",
    "\n",
    "\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, K):\n",
    "        \n",
    "        conv_layer_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "        hidden_layer_sizes = [512]\n",
    "        gamma = 0.99\n",
    "        \n",
    "        self.K = K # Number of possible actions\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, 4, BOARD_SIZE, BOARD_SIZE), name='X') #input\n",
    "        self.G = tf.placeholder(tf.float32, shape=(None,), name='G') #rewards\n",
    "        self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions') #actions\n",
    "        \n",
    "        # normalize input\n",
    "        Z = self.X / 255.0\n",
    "        # permute input dimensions\n",
    "        Z = tf.transpose(Z, [0, 2, 3, 1])\n",
    "\n",
    "        for num_output_filters, num_filters, num_pools in conv_layer_sizes:\n",
    "            Z = tf.contrib.layers.conv2d(Z, num_output_filters, num_filters,\n",
    "                num_pools,activation_fn=tf.nn.relu)\n",
    "\n",
    "        # fully connected layers\n",
    "        Z = tf.contrib.layers.flatten(Z)\n",
    "        for M in hidden_layer_sizes:\n",
    "            Z = tf.contrib.layers.fully_connected(Z, M)\n",
    "\n",
    "        # final output layer\n",
    "        self.predict_op = tf.contrib.layers.fully_connected(Z, K)\n",
    "\n",
    "        selected_action_values = tf.reduce_sum(\n",
    "            self.predict_op * tf.one_hot(self.actions, self.K),\n",
    "            reduction_indices=[1]\n",
    "        )\n",
    "\n",
    "        cost = tf.reduce_mean(tf.square(self.G - selected_action_values))\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-2).minimize(cost)\n",
    "        self.cost = cost\n",
    "    \n",
    "    def predict(self, states):\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: states})\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "    \n",
    "    def update_model(self, states, actions, targets):\n",
    "        loss, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.X: states,\n",
    "                self.G: targets,\n",
    "                self.actions: actions\n",
    "            }\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def select_action(self, obs, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            return np.argmax(self.predict([obs])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined our model for the network, we create some useful methods that we'll be calling iteratively. These methods are for learning through playing episodes. The play_one_episode method plays a full episode, and on each action, updates the network adjusted by the minimization define in the network's class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, experience_buffer, gamma, batch_size):\n",
    "\n",
    "    experience_sample = random.sample(experience_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, game_overs = map(np.array, zip(*experience_sample))\n",
    "    \n",
    "#     print(game_overs)\n",
    "\n",
    "    possibleQs = model.predict(next_states)\n",
    "    selectedQ = np.amax(possibleQs, axis=1)\n",
    "    targets = rewards + np.invert(game_overs).astype(np.float32) * gamma * selectedQ\n",
    "\n",
    "    loss = model.update_model(states, actions, targets)\n",
    "    return loss\n",
    "\n",
    "def play_one_episode(env, experience_buffer, model, gamma, batch_size,\n",
    "                     epsilon, epsilon_delta, epsilon_min):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    scaled_obs = rescale_board(obs)\n",
    "    state = np.stack([scaled_obs] * s, axis=0)\n",
    "    loss = None\n",
    "    episode_reward = 0\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "        action = model.select_action(state, epsilon)\n",
    "        obs, reward, game_over, _ = env.step(action)\n",
    "        scaled_obs = rescale_board(obs)\n",
    "        next_state = np.append(state[1:], np.expand_dims(scaled_obs, 0), axis=0)\n",
    "        \n",
    "        if len(experience_buffer) == MAX_REPLAY_EXPERIENCES:\n",
    "            experience_buffer.pop(0)\n",
    "\n",
    "        experience_buffer.append((state, action, reward, next_state, game_over))\n",
    "\n",
    "        loss = learn(model, experience_buffer, gamma, batch_size)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        epsilon = max(epsilon - epsilon_delta, epsilon_min)\n",
    "\n",
    "    return episode_reward, epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize an instance of this network model. We will play a number of episodes (i.e. whole games before going to game over) and learn from each of those. Each subsequent episode will have an adjusted network from the previous ones. At the beginning, we choose random actions more often (epsilon). As we advance through the game, the choosing of random actions will be decaying because we reduce epsilon linearly. Every time we finish an episode, we'll print the game statistics: how much reward we got for this game, how is the average reward, what epsilon are we using, and in which episode are we at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 170.0 Avg Reward: 170.000 Epsilon: 0.936\n",
      "1\n",
      "Episode: 1 Reward: 310.0 Avg Reward: 240.000 Epsilon: 0.864\n",
      "2\n",
      "Episode: 2 Reward: 530.0 Avg Reward: 336.667 Epsilon: 0.749\n",
      "3\n",
      "Episode: 3 Reward: 420.0 Avg Reward: 357.500 Epsilon: 0.682\n",
      "4\n",
      "Episode: 4 Reward: 260.0 Avg Reward: 338.000 Epsilon: 0.629\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "model = QNetwork(K=env.action_space.n)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.set_session(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        print(i)\n",
    "        episode_reward, epsilon = play_one_episode(env, experience_buffer, model,\n",
    "                                           gamma, batch_size, epsilon, epsilon_delta,\n",
    "                                           epsilon_min)\n",
    "        \n",
    "        episode_rewards[i] = episode_reward\n",
    "\n",
    "        avg_latest = episode_rewards[max(0, i - 100):i + 1].mean()\n",
    "        print(\"Episode:\", i, \"Reward:\", episode_reward,\n",
    "        \"Avg Reward:\", \"%.3f\" % avg_latest, \"Epsilon:\", \"%.3f\" % epsilon )\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
