{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MsPacman Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a model to play Ms Pacman through reinforcement learning. \n",
    "For this purpose, we use the OpenAI Gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "#Use MsPacman\n",
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an agent decission problem (?) we need a set of possible actions the agent can perform. The environment we created has a set of possible actions, and we can see their meaning in the context of an Atari game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, we decide an action, and this brings with it a new state, a reward, if the game is over, and more information on the game (like if we have more lifes left).\n",
    "\n",
    "Let's see what would happen if the agent always chose to go downwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(500):\n",
    "    action = 4\n",
    "    obs, reward, game_over, info = env.step(action)\n",
    "    if reward > 0:\n",
    "        print(reward,game_over,info['ale.lives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what each state looks like in terms of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(len(obs))\n",
    "print(len(obs[0]))\n",
    "print(len(obs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "\n",
    "Before training, we can convert our board to images that actually matter for the game. We can also turn it into a square image, if no information is lost\n",
    "\n",
    "This is how the board looks at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "original_board = env.reset()\n",
    "plt.imshow(original_board)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the lower rectangle that has the game statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = original_board[0:170]\n",
    "plt.imshow(cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make it blurry using nearest neighbour image interpolation to resie the image to half the size. The ghosts are still there, so are the points, pacman and the walls. We can also convert it to gray scale to not have 3 components of RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "scaled = cropped.mean(axis=2) # convert to grayscale\n",
    "scaled = scaled/255 #normalize\n",
    "scaled = imresize(scaled, size=(85,85), interp='nearest')\n",
    "plt.imshow(scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we create our first constant and helping function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOARD_SIZE = 85\n",
    "\n",
    "def rescale_board(board):\n",
    "    cropped = board[0:170]\n",
    "    scaled = cropped.mean(axis=2)\n",
    "    return imresize(scaled/255, size=(BOARD_SIZE, BOARD_SIZE), interp='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition and State Update\n",
    "\n",
    "To train the network, we don't use just the immediate previous screen, we need a set of screens. This way, the network can better make inference on things like speed and direction of moving objects on the game's screen.\n",
    "\n",
    "In the paper used for this project, the authors used 4 screens to define the \"state\". We'll do the same.\n",
    "\n",
    "We define a function to update the state, which will take as an input the current state and the new observation, and it will remove the oldest observation, and introduce the new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_state(current_state, new_obs):\n",
    "    scaled_obs = rescale_board(new_obs)\n",
    "    return np.append(state[1:], np.expand_dims(scaled_obs, 0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Before doing anything else, we'll start creating our buffer for experience replay.\n",
    "\n",
    "Experience replay is an improvement done to deep q-learning. Essentialy, you accumulate experiences (states) from game play. This plays can be done by an expert player, or at random. They are useful because they eliminate the time-dependencies introduced by the fact that the game frames always follow a sequence.\n",
    "\n",
    "We generate our buffer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_REPLAY_EXPERIENCES = 20 # for real 50000, for testing 20\n",
    "MAX_REPLAY_EXPERIENCES = 100 # for real 500000, for testing 100\n",
    "\n",
    "s = 4 # number of screens in a state\n",
    "experience_buffer = []\n",
    "K = env.action_space.n\n",
    "\n",
    "initial_obs = env.reset()\n",
    "scaled_initial_obs = rescale_board(initial_obs)\n",
    "state = np.stack([scaled_initial_obs] * s, axis=0)\n",
    "for i in range(MIN_REPLAY_EXPERIENCES):\n",
    "    action = np.random.choice(K)\n",
    "    obs, reward, game_over, _ = env.step(action)\n",
    "    next_state = update_state(state, obs)\n",
    "    experience_buffer.append((state, action, reward, next_state, game_over))\n",
    "    if game_over:\n",
    "        obs = env.reset()\n",
    "        scaled_obs = rescale_board(obs)\n",
    "        state = np.stack([scaled_obs] * s, axis=0)\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Definition\n",
    "\n",
    "Now we define the Neural Network using tensorflow (RO. ECHALE MAS VERBO AQUI) (G son las recompensas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "## Training params definition\n",
    "batch_size = 10 #for real 30, for testing 10\n",
    "num_episodes = 25 # for real 10000 for testing 25\n",
    "gamma = 0.99\n",
    "episode_rewards = np.zeros(num_episodes)\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_delta = (epsilon - epsilon_min) / 10000 # for real divide by 500000, for testing by 10000\n",
    "\n",
    "\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, K):\n",
    "        \n",
    "        conv_layer_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "        hidden_layer_sizes = [512]\n",
    "        gamma = 0.99\n",
    "        \n",
    "        self.K = K # Number of possible actions\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, 4, BOARD_SIZE, BOARD_SIZE), name='X') #input\n",
    "        self.G = tf.placeholder(tf.float32, shape=(None,), name='G') #rewards\n",
    "        self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions') #actions\n",
    "        \n",
    "        # normalize input\n",
    "        Z = self.X / 255.0\n",
    "        # permute input dimensions\n",
    "        Z = tf.transpose(Z, [0, 2, 3, 1])\n",
    "\n",
    "        for num_output_filters, num_filters, num_pools in conv_layer_sizes:\n",
    "            Z = tf.contrib.layers.conv2d(Z, num_output_filters, num_filters,\n",
    "                num_pools,activation_fn=tf.nn.relu)\n",
    "\n",
    "        # fully connected layers\n",
    "        Z = tf.contrib.layers.flatten(Z)\n",
    "        for M in hidden_layer_sizes:\n",
    "            Z = tf.contrib.layers.fully_connected(Z, M)\n",
    "\n",
    "        # final output layer\n",
    "        self.predict_op = tf.contrib.layers.fully_connected(Z, K)\n",
    "\n",
    "        selected_action_values = tf.reduce_sum(\n",
    "            self.predict_op * tf.one_hot(self.actions, self.K),\n",
    "            reduction_indices=[1]\n",
    "        )\n",
    "\n",
    "        cost = tf.reduce_mean(tf.square(self.G - selected_action_values))\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-2).minimize(cost)\n",
    "        self.cost = cost\n",
    "    \n",
    "    def predict(self, states):\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: states})\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "    \n",
    "    def update_model(self, states, actions, targets):\n",
    "        loss, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.X: states,\n",
    "                self.G: targets,\n",
    "                self.actions: actions\n",
    "            }\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def select_action(self, obs, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            return np.argmax(self.predict([obs])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(model, experience_buffer, gamma, batch_size):\n",
    "\n",
    "    experience_sample = random.sample(experience_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, game_overs = map(np.array, zip(*experience_sample))\n",
    "    \n",
    "    print(game_overs)\n",
    "\n",
    "    possibleQs = model.predict(next_states)\n",
    "    selectedQ = np.amax(possibleQs, axis=1)\n",
    "    targets = rewards + np.invert(game_overs).astype(np.float32) * gamma * selectedQ\n",
    "\n",
    "    # Update model\n",
    "    loss = model.update_model(states, actions, targets)\n",
    "    return loss\n",
    "\n",
    "def play_one_episode(env, experience_buffer, model, gamma, batch_size,\n",
    "                     epsilon, epsilon_delta, epsilon_min):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    scaled_obs = rescale_board(obs)\n",
    "    state = np.stack([scaled_obs] * s, axis=0)\n",
    "    loss = None\n",
    "    episode_reward = 0\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "        # Take action\n",
    "        action = model.select_action(state, epsilon)\n",
    "        obs, reward, game_over, _ = env.step(action)\n",
    "        scaled_obs = rescale_board(obs)\n",
    "        next_state = np.append(state[1:], np.expand_dims(scaled_obs, 0), axis=0)\n",
    "        \n",
    "        if len(experience_buffer) == MAX_REPLAY_EXPERIENCES:\n",
    "            experience_buffer.pop(0)\n",
    "\n",
    "        experience_buffer.append((state, action, reward, next_state, game_over))\n",
    "\n",
    "        loss = learn(model, experience_buffer, gamma, batch_size)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        epsilon = max(epsilon - epsilon_delta, epsilon_min)\n",
    "\n",
    "    return episode_reward, epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(K=env.action_space.n)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.set_session(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Play a number of episodes and learn!\n",
    "    for i in range(num_episodes):\n",
    "        print(i)\n",
    "        episode_reward, epsilon = play_one_episode(env, experience_buffer, model,\n",
    "                                           gamma, batch_size, epsilon, epsilon_delta,\n",
    "                                           epsilon_min)\n",
    "        \n",
    "        episode_rewards[i] = episode_reward\n",
    "\n",
    "        last_100_avg = episode_rewards[max(0, i - 100):i + 1].mean()\n",
    "        print(\"Episode:\", i,\n",
    "        \"Reward:\", episode_reward,\n",
    "        \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg,\n",
    "        \"Epsilon:\", \"%.3f\" % epsilon\n",
    "        )\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
