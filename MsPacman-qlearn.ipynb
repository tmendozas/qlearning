{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MsPacman Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a model to play Ms Pacman through reinforcement learning. \n",
    "For this purpose, we use the OpenAI Gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "#Use MsPacman\n",
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an agent decission problem (?) we need a set of possible actions the agent can perform. The environment we created has a set of possible actions, and we can see their meaning in the context of an Atari game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, we decide an action, and this brings with it a new state, a reward, if the game is over, and more information on the game (like if we have more lifes left).\n",
    "\n",
    "Let's see what would happen if the agent always chose to go downwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(500):\n",
    "    action = 4\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if reward > 0:\n",
    "        print(reward,done,info['ale.lives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what each state looks like in terms of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(len(obs))\n",
    "print(len(obs[0]))\n",
    "print(len(obs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "\n",
    "Before training, we can convert our board to images that actually matter for the game. We can also turn it into a square image, if no information is lost\n",
    "\n",
    "This is how the board looks at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "original_board = env.reset()\n",
    "plt.imshow(original_board)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the lower rectangle that has the game statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = original_board[0:170]\n",
    "plt.imshow(cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make it blurry using nearest neighbour image interpolation to resie the image to half the size. The ghosts are still there, so are the points, pacman and the walls. We can also convert it to gray scale to not have 3 components of RGB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "scaled = cropped.mean(axis=2) # convert to grayscale\n",
    "scaled = scaled/255 #normalize\n",
    "scaled = imresize(scaled, size=(85,85), interp='nearest')\n",
    "plt.imshow(scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we create our first constant and helping function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOARD_SIZE = 85\n",
    "\n",
    "def rescale_board(board):\n",
    "    cropped = board[0:170]\n",
    "    scaled = cropped.mean(axis=2)\n",
    "    return imresize(scaled/255, size=(BOARD_SIZE, BOARD_SIZE), interp='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition and State Update\n",
    "\n",
    "To train the network, we don't use just the immediate previous screen, we need a set of screens. This way, the network can better make inference on things like speed and direction of moving objects on the game's screen.\n",
    "\n",
    "In the paper used for this project, the authors used 4 screens to define the \"state\". We'll do the same.\n",
    "\n",
    "We define a function to update the state, which will take as an input the current state and the new observation, and it will remove the oldest observation, and introduce the new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_state(current_state, new_obs):\n",
    "    scaled_obs = rescale_board(new_obs)\n",
    "    return np.append(state[1:], np.expand_dims(scaled_obs, 0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Before doing anything else, we'll start creating our buffer for experience replay.\n",
    "\n",
    "Experience replay is an improvement done to deep q-learning. Essentialy, you accumulate experiences (states) from game play. This plays can be done by an expert player, or at random. They are useful because they eliminate the time-dependencies introduced by the fact that the game frames always follow a sequence.\n",
    "\n",
    "We generate our buffer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_EXPERIENCES = 500 # for real 50000, for testing 500\n",
    "K = env.action_space.n # Number of possible actions\n",
    "s = 4 # number of screens in a state\n",
    "experience_buffer = []\n",
    "\n",
    "initial_obs = env.reset()\n",
    "scaled_initial_obs = rescale_board(initial_obs)\n",
    "state = np.stack([scaled_initial_obs] * s, axis=0)\n",
    "for i in range(REPLAY_EXPERIENCES):\n",
    "    action = np.random.choice(K)\n",
    "    obs, reward, game_over, _ = env.step(action)\n",
    "    next_state = update_state(state, obs)\n",
    "    experience_buffer.append((state, action, reward, next_state, game_over))\n",
    "    if game_over:\n",
    "        obs = env.reset()\n",
    "        scaled_obs = downsample_image(obs)\n",
    "        state = np.stack([scaled_obs] * s, axis=0)\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some more constants. This time, it's the number of games we'll play to train and the number of experiences for the \"experience-replay\" batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_EXPERIENCES = 5000 #for real 500000, for testing 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 4, IM_SIZE, IM_SIZE), name='X')\n",
    "\n",
    "# tensorflow convolution needs the order to be:\n",
    "# (num_samples, height, width, \"color\")\n",
    "# so we need to tranpose later\n",
    "G = tf.placeholder(tf.float32, shape=(None,), name='G')\n",
    "actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n",
    "\n",
    "# calculate output and cost\n",
    "# convolutional layers\n",
    "# these built-in layers are faster and don't require us to\n",
    "# calculate the size of the output of the final conv layer!\n",
    "Z = X / 255.0\n",
    "# print(Z)\n",
    "Z = tf.transpose(Z, [0, 2, 3, 1])\n",
    "# print(Z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "hidden_layer_sizes = [512]\n",
    "\n",
    "for num_output_filters, filtersz, poolsz in conv_layer_sizes:\n",
    "  Z = tf.contrib.layers.conv2d(\n",
    "      Z,\n",
    "      num_output_filters,\n",
    "      filtersz,\n",
    "      poolsz,\n",
    "      activation_fn=tf.nn.relu\n",
    "    )\n",
    "\n",
    "# fully connected layers\n",
    "Z = tf.contrib.layers.flatten(Z)\n",
    "for M in hidden_layer_sizes:\n",
    "    Z = tf.contrib.layers.fully_connected(Z, M)\n",
    "\n",
    "# final output layer\n",
    "predict_op = tf.contrib.layers.fully_connected(Z, K)\n",
    "\n",
    "selected_action_values = tf.reduce_sum(\n",
    "    predict_op * tf.one_hot(actions, K),\n",
    "    reduction_indices=[1]\n",
    ")\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(G - selected_action_values))\n",
    "train_op = tf.train.AdamOptimizer(1e-2).minimize(cost)\n",
    "cost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
