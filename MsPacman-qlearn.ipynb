{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MsPacman Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a model to play Ms Pacman through reinforcement learning. \n",
    "For this purpose, we use OpenAI Gym library, and use the atari environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-18 21:50:53,024] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "#Use MsPacman\n",
    "env = gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an agent decission problem (?) we need a set of possible actions the agent can perform. The environment we created has a set of possible actions, and we can see their meaning in the context of an Atari game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, we decide an action, and this brings with it a new state, a reward, if the game is over, and more information on the game (like if we have more lifes left).\n",
    "\n",
    "Let's see what would happen if the agent always chose to go downwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n",
      "10.0 False 3\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(1000):\n",
    "    action = 4\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if reward > 0:\n",
    "        print(reward,done,info['ale.lives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ..., \n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ..., \n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "210\n",
      "160\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs)\n",
    "print(len(obs))\n",
    "print(len(obs[0]))\n",
    "print(len(obs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "IM_SIZE = 80\n",
    "K = env.action_space.n\n",
    "X = tf.placeholder(tf.float32, shape=(None, 4, IM_SIZE, IM_SIZE), name='X')\n",
    "\n",
    "# tensorflow convolution needs the order to be:\n",
    "# (num_samples, height, width, \"color\")\n",
    "# so we need to tranpose later\n",
    "G = tf.placeholder(tf.float32, shape=(None,), name='G')\n",
    "actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n",
    "\n",
    "# calculate output and cost\n",
    "# convolutional layers\n",
    "# these built-in layers are faster and don't require us to\n",
    "# calculate the size of the output of the final conv layer!\n",
    "Z = X / 255.0\n",
    "# print(Z)\n",
    "Z = tf.transpose(Z, [0, 2, 3, 1])\n",
    "# print(Z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "hidden_layer_sizes = [512]\n",
    "\n",
    "for num_output_filters, filtersz, poolsz in conv_layer_sizes:\n",
    "  Z = tf.contrib.layers.conv2d(\n",
    "      Z,\n",
    "      num_output_filters,\n",
    "      filtersz,\n",
    "      poolsz,\n",
    "      activation_fn=tf.nn.relu\n",
    "    )\n",
    "\n",
    "# fully connected layers\n",
    "Z = tf.contrib.layers.flatten(Z)\n",
    "for M in hidden_layer_sizes:\n",
    "    Z = tf.contrib.layers.fully_connected(Z, M)\n",
    "\n",
    "# final output layer\n",
    "predict_op = tf.contrib.layers.fully_connected(Z, K)\n",
    "\n",
    "selected_action_values = tf.reduce_sum(\n",
    "    predict_op * tf.one_hot(actions, K),\n",
    "    reduction_indices=[1]\n",
    ")\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(G - selected_action_values))\n",
    "# self.train_op = tf.train.AdamOptimizer(1e-2).minimize(cost)\n",
    "# self.train_op = tf.train.AdagradOptimizer(1e-2).minimize(cost)\n",
    "# self.train_op = tf.train.RMSPropOptimizer(2.5e-4, decay=0.99, epsilon=10e-3).minimize(cost)\n",
    "train_op = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6).minimize(cost)\n",
    "# self.train_op = tf.train.MomentumOptimizer(1e-3, momentum=0.9).minimize(cost)\n",
    "# self.train_op = tf.train.GradientDescentOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "cost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
